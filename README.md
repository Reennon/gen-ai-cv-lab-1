# gen-ai-cv-lab-1

This repository implements a modular training pipeline for generative models on the CIFAR-10 dataset. It supports Autoencoders, Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Normalizing Flows. The pipeline is designed for flexibility, reproducibility, and scalability, with robust logging and visualization tools.

### Training Notebooks:

#### Autoencoder:
Autoencoder proved to be a simple and robust DNN to extract the features and represent the edges and general silhouette of the original image.   

```notebooks/autoencoder.ipynb```
link to wandb (please wait a few moments it's taking a while to load all of those images):
https://wandb.ai/rkovalchuk/cifar-10-autoencoder?nw=nwuserrkovalch

#### VAE:
For me result in VAE is very similar to Autoencoder, with the only difference being is that convergence and training was much easier. Also results and silhouettes resembled original images more than AE.
https://wandb.ai/rkovalchuk/cifar-10-vae?nw=nwuserrkovalch

#### GAN:
link to wandb (the best experiment is gan-600-Lipschitz-Penalty-lp-9.5 with 600 epochs): 
https://wandb.ai/rkovalchuk/cifar-10-gan?nw=nwuserrkovalch
- I've also incorporated Lipschitz Penalty for stability in training and validation losses which has helped me to mitigate domination of the discriminator and generator before manually tuning this parameter. However the most limiting factor in my opinion was the architecture itself, and the enormous training time it takes to start producing even minimal meaningful results. However, it's the most robust out of the first 3, except GEN with Norm Flow, however I have only trained the latter for 150 epochs only, due to limited time I've had. 

##### GAN-wider is a version of GAN where I extended the generator to 512 neurons wide from 256 in the second layer, and lowered the latent dim to 150
- The results are not very different from the original GAN, and the experimentation is limited due to the enormous time it takes to train a simple GAN which produced meaningful results.

##### Norm Flow:
Norm Flow is a prospect architecture which converges a lot faster than GAN as it seems to me, as in its nature it's a GAN but with Augmentations (Normalized Flow).

https://wandb.ai/rkovalchuk/cifar-10-norm-flow?nw=nwuserrkovalch

### FAQ

##### How to open in Colab?
> There's button at the beginning of each notebook for each model
##### How to download a model?
> There's an artifact with best model saved for each run in wandb for each model architecture

##### How to install dependencies in Google Colab?
> Follow those two steps:
```shell
!git clone https://github.com/Reennon/gen-ai-cv-lab-1.git
!pip install -r requirements.txt
```
##### For poetry setup:
```shell
poetry install
```

##### Generated Image Comparison

---

### **Image Comparison Table**

| **Original Image**                              | **Autoencoder**                              | **VAE**                                      | **GAN**                                     | **Normalizing Flow**                         |
|------------------------------------------------|----------------------------------------------|----------------------------------------------|---------------------------------------------|----------------------------------------------|
| ![real_images.png](images%2Freal_images.png)       |![ae.png](images%2Fae.png) | ![vae.png](images%2Fvae.png) | ![gan.png](images%2Fgan.png) | ![norm_flow.png](images%2Fnorm_flow.png) |

* Gan has been trained for 600 epochs, and Normalized Flow seems underfitted, but superbly show a general state of the things, how the images were generated by such type of model

---

### **How to Fill the Table**

1. **Original Image**:
   - Use a batch of original images from the CIFAR-10 dataset validation set.
   - Save the combined batch of images as `original.png` and `original2.png`.

2. **Reconstructed/Generated Images**:
   - Save reconstructed or generated

## Directory Structure
```text
gen-ai-cv-lab-1/
│
├── data/                     # CIFAR-10 dataset storage
├── notebooks/                # Notebooks for each model
│   ├── autoencoder.ipynb     # Autoencoder training and visualization
│   ├── gan.ipynb             # GAN training and visualization
│   ├── gan_wider.ipynb       # Wider GAN architecture experiments
│   ├── norm_flow.ipynb       # Normalizing Flow experiments
│   └── vae.ipynb             # VAE training and visualization
├── params/                   # Model-specific YAML configuration files
│   ├── autoencoder.yml
│   ├── gan.yml
│   ├── norm_flow.yml
│   └── vae.yml
├── src/                      # Source code
│   ├── models/               # Model definitions
│   │   ├── autoencoder.py
│   │   ├── base_model.py     # Shared functionality for all models
│   │   ├── gan.py
│   │   ├── norm_flow.py
│   │   └── vae.py
│   ├── training/             # Training and evaluation logic
│   │   └── trainer.py
│   └── visualization/        # Visualization tools
│       ├── base_visualizer.py
│       ├── gan_visualizer.py
│       └── vae_visualizer.py
├── .env                      # Environment variables
├── .gitignore
├── README.md                 # Project documentation
├── requirements.txt          # Python dependencies
├── sample.env                # Example environment variable file
└── pyproject.toml            # Poetry configuration
```
